{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl2r0KYuMSDoPFkCUy7mK9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parateja/teja/blob/main/NLP_INTERNAL_AND_EXTERNAL_PROGRAMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "St4BIutUdFZ7",
        "outputId": "bab8a467-8878-4f27-defd-4856f750de52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " my name is jithendra teja i am studying B.tech third year\n",
            " my name is jithendra teja i am studying B.tech third year\n",
            " \n",
            "my\n",
            "name\n",
            "is\n",
            "jithendra\n",
            "teja\n",
            "i\n",
            "am\n",
            "studying\n",
            "B.tech\n",
            "third\n",
            "year\n"
          ]
        }
      ],
      "source": [
        "#exp-1(word tokenization)\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(\" my name is jithendra teja i am studying B.tech third year\")\n",
        "\n",
        "print(doc)\n",
        "for s in doc.sents:\n",
        "  print(s)\n",
        "\n",
        "for s in doc.sents:\n",
        "  for w in s:\n",
        "    print(w)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-2(word generation)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "text=\"the boy is drinking milk\"\n",
        "word=word_tokenize(text)\n",
        "print(word)\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "tags=nltk.pos_tag(word)\n",
        "print(tags)\n",
        "\n",
        "# parse tree construction\n",
        "\n",
        "from nltk import pos_tag, word_tokenize, RegexpParser\n",
        "\n",
        "chunker = RegexpParser(\"\"\"\n",
        "                    NP: {<DT>?<JJ>*<NN>} #To extract Noun Phrases\n",
        "                    P: {<IN>}            #To extract Prepositions\n",
        "                    V: {<V.*>}           #To extract Verbs\n",
        "                    PP: {<p> <NP>}       #To extract Prepositional Phrases\n",
        "                    VP: {<V> <NP|PP>*}   #To extract Verb Phrases\n",
        "                    \"\"\")\n",
        "\n",
        "output = chunker.parse(tags)\n",
        "print(output)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XVQ9gxIOdstN",
        "outputId": "46f3ba7a-7f85-4575-899f-3724dac17fbb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'boy', 'is', 'drinking', 'milk']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 'DT'), ('boy', 'NN'), ('is', 'VBZ'), ('drinking', 'VBG'), ('milk', 'NN')]\n",
            "(S\n",
            "  (NP the/DT boy/NN)\n",
            "  (VP (V is/VBZ))\n",
            "  (VP (V drinking/VBG) (NP milk/NN)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-3(morphological analysis)\n",
        "\n",
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(\"the dogs are barking\")\n",
        "for token in doc:\n",
        "  print(token,\"|\",token.morph)\n",
        "  print(token,\"|\",token.pos)\n",
        "  print(token,\"|\",token.pos_)\n",
        "  print(token,\"|\",token.tag_)\n",
        "  print(token,\"|\",spacy.explain(token.tag_))\n",
        "  print(token,\"|\",spacy.explain(token.tag))\n",
        "#stemming\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer=SnowballStemmer(language=\"english\")\n",
        "token=input(\"enter a token: \")\n",
        "stemmer.stem(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "6JUHjO8veO7n",
        "outputId": "f75c2ad4-8c45-41ae-d973-095d0847b3a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/glossary.py:20: UserWarning: [W118] Term '15267657372422890137' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
            "  warnings.warn(Warnings.W118.format(term=term))\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/glossary.py:20: UserWarning: [W118] Term '783433942507015291' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
            "  warnings.warn(Warnings.W118.format(term=term))\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/glossary.py:20: UserWarning: [W118] Term '9188597074677201817' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
            "  warnings.warn(Warnings.W118.format(term=term))\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/glossary.py:20: UserWarning: [W118] Term '1534113631682161808' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
            "  warnings.warn(Warnings.W118.format(term=term))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the | Definite=Def|PronType=Art\n",
            "the | 90\n",
            "the | DET\n",
            "the | DT\n",
            "the | determiner\n",
            "the | None\n",
            "dogs | Number=Plur\n",
            "dogs | 92\n",
            "dogs | NOUN\n",
            "dogs | NNS\n",
            "dogs | noun, plural\n",
            "dogs | None\n",
            "are | Mood=Ind|Tense=Pres|VerbForm=Fin\n",
            "are | 87\n",
            "are | AUX\n",
            "are | VBP\n",
            "are | verb, non-3rd person singular present\n",
            "are | None\n",
            "barking | Aspect=Prog|Tense=Pres|VerbForm=Part\n",
            "barking | 100\n",
            "barking | VERB\n",
            "barking | VBG\n",
            "barking | verb, gerund or present participle\n",
            "barking | None\n",
            "enter a token: dogs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-4(N-gram)\n",
        "\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "nltk.download('punkt')\n",
        "samplText='this is a very good book to study'\n",
        "NGRAMS=ngrams(sequence=nltk.word_tokenize(samplText), n=3)\n",
        "for grams in NGRAMS:\n",
        "    print(grams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7aLVztTJekfe",
        "outputId": "5aa91535-53ca-4be1-f7f4-df468a2981b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('this', 'is', 'a')\n",
            "('is', 'a', 'very')\n",
            "('a', 'very', 'good')\n",
            "('very', 'good', 'book')\n",
            "('good', 'book', 'to')\n",
            "('book', 'to', 'study')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exp-5(N-gram smoothing)\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "# main module begin here\n",
        "\n",
        "sentences = \"This sntence contins errors. This sentence has to be corrcted.\"\n",
        "list_string = sentences.split(' ')\n",
        "for word in list_string:\n",
        "    print(correction(word))\n",
        "\n",
        "\n",
        "print(' '.join('<<'+i+'>>'+' %s'%correction(i) if correction(i) != i else i for i in sentences.split()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9pJkMlxBe8es",
        "outputId": "7c176944-cc2f-48b7-9185-874a801c3da4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this\n",
            "sntence\n",
            "contins\n",
            "errors\n",
            "this\n",
            "sentence\n",
            "has\n",
            "to\n",
            "be\n",
            "corrcted\n",
            "<<This>> this sntence contins <<errors.>> errors <<This>> this sentence has to be <<corrcted.>> corrcted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exp-6(hidden morkov model)\n",
        "\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Load the Brown corpus\n",
        "corpus = brown.tagged_sents()\n",
        "\n",
        "# Split corpus into train and test sets\n",
        "train_corpus = corpus[:int(0.8*len(corpus))]\n",
        "test_corpus = corpus[int(0.8*len(corpus)):]\n",
        "\n",
        "# Define the tagset\n",
        "tagset = set([tag for sentence in corpus for _, tag in sentence])\n",
        "\n",
        "# Define the HMM model\n",
        "trainer = nltk.tag.hmm.HiddenMarkovModelTrainer(tagset)\n",
        "\n",
        "# Train the model on the training set\n",
        "model = trainer.train_supervised(train_corpus)\n",
        "\n",
        "# Test the model on the test set\n",
        "accuracy = model.accuracy(test_corpus)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Use the model to tag a new sentence\n",
        "new_sentence = \"The quick brown fox jumps over the lazy dog\".split()\n",
        "tagged_sentence = model.tag(new_sentence)\n",
        "print(tagged_sentence)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "owWPoYeTf1Od",
        "outputId": "6f26e8ed-1fb4-4608-968d-d10d45b0b6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-7(viterbi algorithm)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def viterbi_decode(obs, states, start_p, trans_p, emit_p):\n",
        "    # Initialize variables\n",
        "    N = len(obs)\n",
        "    T = len(states)\n",
        "    path = np.zeros((N, T))\n",
        "    prob = np.zeros((N, T))\n",
        "    path[0, :] = 0\n",
        "    prob[0, :] = start_p * emit_p[:, obs[0]]\n",
        "\n",
        "    # Run Viterbi algorithm\n",
        "    for t in range(1, N):\n",
        "        for j in range(T):\n",
        "            prob[t, j] = np.max(prob[t-1, :] * trans_p[:, j] * emit_p[j, obs[t]])\n",
        "            path[t, j] = np.argmax(prob[t-1, :] * trans_p[:, j])\n",
        "\n",
        "    # Backtrack to find the most likely sequence of states\n",
        "    best_seq = np.zeros(N)\n",
        "    best_seq[-1] = np.argmax(prob[-1, :])\n",
        "    for t in range(N-2, -1, -1):\n",
        "        best_seq[t] = path[t+1, int(best_seq[t+1])]\n",
        "\n",
        "    # Convert the sequence of state indices to labels\n",
        "    best_labels = [states[int(i)] for i in best_seq]\n",
        "    return best_labels\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"I like pizza\"\n",
        "\n",
        "# Example POS tags and their corresponding indices\n",
        "tags = ['PRP', 'VBP', 'NN']\n",
        "tag_indices = {word: i for i, word in enumerate(sentence.split())}\n",
        "tag_indices.update({tag: i for i, tag in enumerate(tags)})\n",
        "\n",
        "# Example transition probabilities\n",
        "trans_p = np.array([\n",
        "    [0.4, 0.6, 0.0],\n",
        "    [0.0, 0.4, 0.6],\n",
        "    [0.0, 0.0, 1.0],\n",
        "])\n",
        "\n",
        "# Example emission probabilities\n",
        "emit_p = np.array([\n",
        "    [0.1, 0.4, 0.5],\n",
        "    [0.0, 0.9, 0.1],\n",
        "    [0.0, 0.0, 1.0],\n",
        "])\n",
        "\n",
        "# Convert the sentence to a list of word indices\n",
        "obs = [tag_indices[word] for word in sentence.split()]\n",
        "\n",
        "# Run Viterbi decoding\n",
        "tags = viterbi_decode(obs, tags, np.array([1.0, 0.0, 0.0]), trans_p, emit_p)\n",
        "\n",
        "# Print the resulting tags\n",
        "print(tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3_M0XHhc0mXk",
        "outputId": "a9b8c762-547f-48ed-e408-acd439acb3be"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PRP', 'VBP', 'NN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-8(penn tree bank)\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the Penn Treebank corpus and the Punkt tokenizer (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Given sentence\n",
        "sentence = \"The tagger produced good results.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Print the POS tagged tokens\n",
        "print(\"POS tagged tokens:\")\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "S6erdwSAnvZJ",
        "outputId": "67c9f056-c38e-4e89-986b-b700ad175d1d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagged tokens:\n",
            "[('The', 'DT'), ('tagger', 'NN'), ('produced', 'VBD'), ('good', 'JJ'), ('results', 'NNS'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-9(chunked text)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "sample_text= \"I am a coding ninja, and I am the best in coding.\"\n",
        "import os\n",
        "os.system('Xvfb :1 -screen 0 1600x1200x16 &') # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
        "os.environ['DISPLAY']=':1.0' # tell X clients to use our virtual DISPLAY :1.0.\n",
        "%matplotlib inline\n",
        "from nltk.tree import Tree\n",
        "from IPython.display import display\n",
        "tokenized=nltk.sent_tokenize(sample_text)\n",
        "for i in tokenized:\n",
        " words=nltk.word_tokenize(i)\n",
        " tagged_words=nltk.pos_tag(words)\n",
        " print(tagged_words)\n",
        " chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\" # this is the grammar that we define,\n",
        " chunkParser=nltk.RegexpParser(chunkGram)\n",
        " chunked=chunkParser.parse(tagged_words)\n",
        " tree = Tree.fromstring(str(chunked))\n",
        " display(tree)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "d65xFkffqU5e",
        "outputId": "0c34c7ea-15fe-4bf8-b3cf-c164fc380866"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('coding', 'NN'), ('ninja', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('am', 'VBP'), ('the', 'DT'), ('best', 'JJS'), ('in', 'IN'), ('coding', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tree('S', ['I/PRP', 'am/VBP', 'a/DT', 'coding/NN', 'ninja/NN', ',/,', 'and/CC', 'I/PRP', 'am/VBP', 'the/DT', 'best/JJS', 'in/IN', 'coding/NN', './.'])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"72px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,888.0,72.0\" width=\"888px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"6.30631%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I/PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.15315%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.20721%\" x=\"6.30631%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">am/VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.90991%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.40541%\" x=\"13.5135%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a/DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.2162%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.90991%\" x=\"18.9189%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">coding/NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.8739%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.00901%\" x=\"28.8288%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ninja/NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.3333%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.5045%\" x=\"37.8378%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,/,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.0901%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.20721%\" x=\"42.3423%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and/CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.9459%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.30631%\" x=\"49.5495%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I/PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.7027%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.20721%\" x=\"55.8559%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">am/VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.4595%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.20721%\" x=\"63.0631%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the/DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.6667%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.00901%\" x=\"70.2703%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">best/JJS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.7748%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.30631%\" x=\"79.2793%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in/IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.4324%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.90991%\" x=\"85.5856%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">coding/NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.5405%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.5045%\" x=\"95.4955%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">./.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.7477%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-10(chunk parser)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('conll2000')\n",
        "from nltk.corpus import conll2000\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from nltk import ChunkParserI, TrigramTagger\n",
        "import random\n",
        "\n",
        "# Load Conll2000 chunked sentences\n",
        "chunked_sentences = list(conll2000.chunked_sents())\n",
        "\n",
        "# Shuffle the sentences\n",
        "random.shuffle(chunked_sentences)\n",
        "\n",
        "# Split into train and test sets\n",
        "train_sents = chunked_sentences[:int(len(chunked_sentences) * 0.9)]\n",
        "test_sents = chunked_sentences[int(len(chunked_sentences) * 0.9 + 1):]\n",
        "\n",
        "# Define a TrigramChunkParser\n",
        "class TrigramChunkParser(ChunkParserI):\n",
        "    def __init__(self, train_sents):\n",
        "        # Extract only the (POS-TAG, IOB-CHUNK-TAG) pairs\n",
        "        train_data = [[(pos_tag, chunk_tag) for word, pos_tag, chunk_tag in tree2conlltags(sent)]\n",
        "                      for sent in train_sents]\n",
        "        # Train a TrigramTagger\n",
        "        self.tagger = TrigramTagger(train_data)\n",
        "\n",
        "    def parse(self, sentence):\n",
        "        pos_tags = [pos for word, pos in sentence]\n",
        "        # Get the Chunk tags\n",
        "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
        "        # Assemble the (word, pos, chunk) triplets\n",
        "        conlltags = [(word, pos_tag, chunk_tag)\n",
        "                     for ((word, pos_tag), (pos_tag, chunk_tag)) in zip(sentence, tagged_pos_tags)]\n",
        "        # Transform to tree\n",
        "        return conlltags2tree(conlltags)\n",
        "\n",
        "# Initialize TrigramChunkParser\n",
        "trigram_chunker = TrigramChunkParser(train_sents)\n",
        "\n",
        "# Evaluate the chunker\n",
        "print(trigram_chunker.evaluate(test_sents))\n",
        "\n",
        "# Example usage\n",
        "sentence = \"He’s really into the spooky decorative light fixture.\"\n",
        "tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
        "parsed_sentence = trigram_chunker.parse(tagged_sentence)\n",
        "print(parsed_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "w6XjW3lYxtk1",
        "outputId": "2cd69f4d-5702-4769-927d-d970d9fba325"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "<ipython-input-43-930250a22f9b>:43: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  print(trigram_chunker.evaluate(test_sents))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  88.1%%\n",
            "    Precision:     80.4%%\n",
            "    Recall:        84.8%%\n",
            "    F-Measure:     82.5%%\n",
            "(S\n",
            "  (NP He/PRP)\n",
            "  (VP ’/VBZ)\n",
            "  s/JJ\n",
            "  really/RB\n",
            "  (PP into/IN)\n",
            "  (NP the/DT spooky/JJ decorative/JJ light/NN fixture/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ]
}